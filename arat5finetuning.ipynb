{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{"id":"vr-nvX_HT4_K"}},{"cell_type":"markdown","source":"The entire code is written using **PyTorch**.<br>\nWe'll be using the **transformers** library by [huggingface](https://github.com/huggingface/transformers) as they provide wrappers for multiple Transformer models.","metadata":{"id":"GqOpA5x73pYs"}},{"cell_type":"code","source":"%load_ext autoreload\n%autoreload 2","metadata":{"execution":{"iopub.status.busy":"2023-08-31T17:21:00.250889Z","iopub.execute_input":"2023-08-31T17:21:00.251605Z","iopub.status.idle":"2023-08-31T17:21:00.302626Z","shell.execute_reply.started":"2023-08-31T17:21:00.251568Z","shell.execute_reply":"2023-08-31T17:21:00.301734Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"%%capture\n\n!pip install transformers\n!pip install pytorch-lightning --upgrade\n!pip install sentencepiece\n!pip install datasets --upgrade\n!pip install torchmetrics\n!pip install wandb --upgrade\n!pip install lightning\n!pip install optuna\n!pip install huggingface-hub","metadata":{"id":"9J7Ws11-9PqG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"iscolab = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if iscolab:\n    from google.colab import files\n    files.upload()\n    ! mkdir ~/.kaggle\n    ! cp kaggle.json ~/.kaggle/\n    ! chmod 600 ~/.kaggle/kaggle.json\n    ! kaggle datasets download -d sifalklioui/hatespeechdza\n    !mkdir data\n    !unzip hatespeechdza.zip -d ./data","metadata":{"id":"61f6obyD_Kud","outputId":"63cd297c-7f38-4c70-d1fa-1b644585edc1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset as hgdataset\nfrom datasets import load_dataset\nimport numpy as np\nimport pickle\nimport seaborn as sns\nimport re\nimport copy\nfrom tqdm.notebook import tqdm\nimport gc\nimport random\nimport torch\nimport wandb\nfrom sklearn import metrics\nfrom huggingface_hub import PyTorchModelHubMixin\nimport torchmetrics\nimport logging\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom dataclasses import dataclass\nfrom sklearn.metrics import f1_score\nimport pytorch_lightning as pl\nfrom torch.optim import AdamW\nfrom torchmetrics import Metric\nfrom lightning.pytorch.loggers import WandbLogger\n\n\n\nfrom transformers import (\n    T5Tokenizer,\n    T5Model,\n    T5ForConditionalGeneration,\n    get_linear_schedule_with_warmup\n)\nwandb_logger = WandbLogger(project=\"HTarabT5\")","metadata":{"id":"07GPLpCt_AjQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass Config:\n    seed = 203\n    data_folder = \"../input/hatespeechdza\"\n    output_dir = './logs'\n    model_name_or_path = 'UBC-NLP/AraT5v2-base-1024'\n    src_max_length = 150\n    tgt_max_length = 2\n    add_special_tokens = True\n    truncation = True\n    return_tensors = 'pt'\n    padding = \"max_length\"\n    weight_decay=0.0\n    adam_epsilon=1e-8\n    warmup_steps=0\n    train_batch_size=16\n    eval_batch_size=16\n    num_train_epochs=2\n    gradient_accumulation_steps=16\n    n_gpu=1\n    fp_16= False, # if you want to enable 16-bit training then install apex and set this to true\n    max_grad_norm= 1 # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n    learning_rate= float(3e-5)\n\nconfig = Config()\n\npl.seed_everything(config.seed)","metadata":{"id":"281L6wVJ6oHj","execution":{"iopub.status.busy":"2023-08-31T17:21:57.244633Z","iopub.execute_input":"2023-08-31T17:21:57.245419Z","iopub.status.idle":"2023-08-31T17:21:57.426455Z","shell.execute_reply.started":"2023-08-31T17:21:57.245383Z","shell.execute_reply":"2023-08-31T17:21:57.425606Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"203"},"metadata":{}}]},{"cell_type":"markdown","source":"# 1.  Dataset & Dataloader","metadata":{"id":"e1AhmXnXByg5"}},{"cell_type":"markdown","source":"Now, we'll create a custom Dataset class inherited from the PyTorch Dataset class. We'll be using the **T5 tokenizer** that returns **input_ids** and **attention_mask**.<br><br>\nThe custom Dataset class will return a dict containing - <br>\n\n- src_input_ids\n- src_attention_mask\n- tgt_input_ids'\n-tgt_attention_mask","metadata":{"id":"cwrzxPV8B1Es"}},{"cell_type":"code","source":"class HateDetect():\n    def __init__(self,config,tokenizer, part):\n\n        self.config = config\n        self.part = part\n        self.tokenizer = tokenizer\n\n\n        data_paths = {\n            'train': config.data_folder + \"/dataset_prep_train.csv\",\n            'test': config.data_folder + \"/dataset_prep_test.csv\",\n            'val': config.data_folder + \"/dataset_prep_val.csv\"\n        }\n        path = data_paths.get(self.part,None)\n        if path is not None:\n            df = pd.read_csv(path)\n            df['label'].replace({0:\"normal\",1:\"hate\"}, inplace = True)\n            self.data = hgdataset.from_pandas(df ,split=self.part)\n        else:\n            raise ValueError(\"Invalid value for self.part\")\n\n\n        self.dataset_scr,self.dataset_tgt = self.tokenize()\n\n        # create funtion to tokenize data\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self,idx):\n\n        source_ids = self.dataset_scr[\"input_ids\"][idx].squeeze()\n        target_ids = self.dataset_tgt[\"input_ids\"][idx].squeeze()\n\n        src_mask    = self.dataset_scr[\"attention_mask\"][idx].squeeze()\n        target_mask = self.dataset_tgt[\"attention_mask\"][idx].squeeze()\n\n        return {\"source_ids\": source_ids,\n                \"source_mask\": src_mask,\n                \"target_ids\": target_ids,\n                \"target_mask\": target_mask}\n\n\n    def tokenize(self):\n\n        tokenizer_params = {\n            \"src\": {\n                \"max_length\": self.config.src_max_length,\n                \"add_special_tokens\": self.config.add_special_tokens,\n                \"truncation\": self.config.truncation,\n                \"return_tensors\": self.config.return_tensors,\n                \"padding\": self.config.padding\n            },\n            \"tgt\": {\n                \"max_length\": self.config.tgt_max_length,\n                \"add_special_tokens\": self.config.add_special_tokens,\n                \"truncation\": self.config.truncation,\n                \"return_tensors\": self.config.return_tensors,\n                \"padding\": self.config.padding\n            }\n        }\n        dataset_scr = self.tokenizer(self.data['text'], **tokenizer_params[\"src\"])\n        dataset_tgt = self.tokenizer(self.data['label'], **tokenizer_params[\"tgt\"])\n        return dataset_scr,dataset_tgt\n\ndef get_dataset(config,tokenizer,part):\n    return HateDetect(config,tokenizer,part)","metadata":{"id":"w0D46GkV6oHl","execution":{"iopub.status.busy":"2023-08-31T17:21:57.430562Z","iopub.execute_input":"2023-08-31T17:21:57.432795Z","iopub.status.idle":"2023-08-31T17:21:57.602461Z","shell.execute_reply.started":"2023-08-31T17:21:57.432758Z","shell.execute_reply":"2023-08-31T17:21:57.601596Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"checkpoint_callback = pl.callbacks.ModelCheckpoint(\n    dirpath= config.output_dir, monitor=\"f1_valid_epoch\", mode=\"max\", save_top_k=1)\n    \ntrain_params = dict(\n    devices=config.n_gpu,\n    strategy=\"auto\",\n    accelerator=\"gpu\",\n    max_epochs=config.num_train_epochs+8,\n    precision= \"16-mixed\" if config.fp_16 else 32,\n    gradient_clip_val=config.max_grad_norm,\n    callbacks=[checkpoint_callback]\n)","metadata":{"id":"U9tLgD2O6oHl","execution":{"iopub.status.busy":"2023-08-31T17:25:34.817477Z","iopub.execute_input":"2023-08-31T17:25:34.817939Z","iopub.status.idle":"2023-08-31T17:25:34.952330Z","shell.execute_reply.started":"2023-08-31T17:25:34.817906Z","shell.execute_reply":"2023-08-31T17:25:34.948808Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# 2. Model","metadata":{"id":"feB5OEdeoV91"}},{"cell_type":"markdown","source":"Coming to the most interesting part - the model architecture! We'll create a class named **Model**, inherited from **torch.nn.Module**.<br><br>\n\n### Flow\n- We initialize our pretrained T5 model with a Conditional Generation Head.\n- Pass in the src & tgt, input_ids & attention_mask.\n- The model returns the decoder generated output ids (predicted labels in textual format), which we need to decode further using the tokenizer.","metadata":{"id":"HjqZhSB8C0Qk"}},{"cell_type":"code","source":"class MyAccuracy(Metric):\n    def __init__(self):\n        super().__init__()\n        higher_is_better = True\n        self.add_state(\"correct\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n\n    def update(self, preds, target):\n        assert preds.shape == target.shape\n\n        self.correct += torch.sum(preds == target)\n        self.total += target.numel()\n\n    def compute(self):\n        return self.correct.float() / self.total","metadata":{"execution":{"iopub.status.busy":"2023-08-31T17:21:57.780483Z","iopub.execute_input":"2023-08-31T17:21:57.782703Z","iopub.status.idle":"2023-08-31T17:21:57.946788Z","shell.execute_reply.started":"2023-08-31T17:21:57.782669Z","shell.execute_reply":"2023-08-31T17:21:57.945788Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class FRP(Metric):\n    def __init__(self):\n        super().__init__()\n        higher_is_better = True\n        self.add_state(\"true_positives\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n        self.add_state(\"false_positives\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n        self.add_state(\"false_negatives\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n\n    def update(self, preds, target):\n        assert preds.shape == target.shape\n\n        self.true_positives += torch.sum((preds == 52459) & (target == 52459))\n        self.false_positives += torch.sum((preds == 52459) & (target == 16147))\n        self.false_negatives += torch.sum((preds == 16147) & (target == 52459))\n\n    def compute(self):\n        precision = self.true_positives.float() / (self.true_positives + self.false_positives).float()\n        recall = self.true_positives.float() / (self.true_positives + self.false_negatives).float()\n\n        f1_score = 2 * (precision * recall) / (precision + recall)\n        return f1_score, recall, precision\n","metadata":{"execution":{"iopub.status.busy":"2023-08-31T17:22:30.141908Z","iopub.execute_input":"2023-08-31T17:22:30.142297Z","iopub.status.idle":"2023-08-31T17:22:30.274858Z","shell.execute_reply.started":"2023-08-31T17:22:30.142268Z","shell.execute_reply":"2023-08-31T17:22:30.273801Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class T5FineTuner(pl.LightningModule,PyTorchModelHubMixin):\n    def __init__(self, config):\n        super().__init__()\n        gc.collect()\n        torch.cuda.empty_cache() \n        self.config = config\n        self.model = T5ForConditionalGeneration.from_pretrained(config.model_name_or_path)\n        self.tokenizer = T5Tokenizer.from_pretrained(config.model_name_or_path)\n        self.save_hyperparameters()\n        self.valid_acc = MyAccuracy()\n        self.FRP = FRP()\n        self.training_step_outputs = []\n        self.validation_step_outputs = []\n\n    def forward(\n        self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, labels=None\n        ):\n        return self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            decoder_attention_mask=decoder_attention_mask,\n            labels=labels,\n            )\n\n    def _step(self, batch):\n        lm_labels = batch[\"target_ids\"]\n        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n        outputs = self(\n        input_ids=batch[\"source_ids\"],\n        attention_mask=batch[\"source_mask\"],\n        labels=lm_labels,\n        decoder_attention_mask=batch['target_mask']\n        )\n        del lm_labels\n        return outputs[0]\n\n    def training_step(self, batch, batch_idx):\n        loss = self._step(batch)\n        #self.log(\"train/loss\", loss)\n        self.training_step_outputs.append(loss.item())\n        return loss\n\n    def on_train_epoch_end(self):\n        \n        self.log(\"training_epoch_average\", np.mean(self.training_step_outputs), sync_dist=True, prog_bar=True, logger=True, on_epoch=True)\n        self.training_step_outputs.clear()  # free memory\n        \n    def validation_step(self, batch, batch_idx):\n        \n        pred_ids = self.model.generate(input_ids=batch['source_ids'],\n                                       attention_mask=batch['source_mask'],\n                                       max_length=2)\n        \n        target_ids = batch['target_ids'][:,0].flatten()\n        self.valid_acc.update(pred_ids[:,1].flatten(),target_ids)\n        self.FRP.update(pred_ids[:,1].flatten(),target_ids)\n        del pred_ids\n        del target_ids\n        \n    def on_validation_epoch_end(self):\n        f1, recall, precision = self.FRP.compute()\n        self.log(\"acc_valid_epoch\", self.valid_acc.compute(), sync_dist=True, prog_bar=True, logger=True, on_epoch=True)\n        self.log(\"f1_valid_epoch\", f1, sync_dist=True, prog_bar=True, logger=True, on_epoch=True)  \n        self.log(\"recall_valid_epoch\", recall, sync_dist=True, prog_bar=True, logger=True, on_epoch=True) \n        self.log(\"precision_valid_epoch\", precision, sync_dist=True, prog_bar=True, logger=True, on_epoch=True) \n        self.valid_acc.reset()\n        self.FRP.reset()\n        del f1\n        del recall\n        del precision\n    def configure_optimizers(self):\n        \"Prepare optimizer and schedule (linear warmup and decay)\"\n\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n\n        optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": self.config.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n        ]\n        optimizer = AdamW(optimizer_grouped_parameters, lr=self.config.learning_rate, eps=self.config.adam_epsilon)\n        scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=self.config.warmup_steps, num_training_steps=self.trainer.estimated_stepping_batches)\n\n        return [optimizer],[scheduler]\n\n\n    def train_dataloader(self):\n        return DataLoader(get_dataset(config=self.config, tokenizer=self.tokenizer, part=\"train\"), batch_size=self.config.train_batch_size, drop_last=True, shuffle=True,num_workers=0)\n    def val_dataloader(self):\n        return DataLoader(get_dataset(config=self.config,tokenizer=self.tokenizer, part=\"val\"), batch_size=self.config.eval_batch_size, drop_last=True,num_workers=0)\n","metadata":{"id":"d1xG8CCdlgge","_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-08-31T17:22:33.071713Z","iopub.execute_input":"2023-08-31T17:22:33.072119Z","iopub.status.idle":"2023-08-31T17:22:33.217249Z","shell.execute_reply.started":"2023-08-31T17:22:33.072089Z","shell.execute_reply":"2023-08-31T17:22:33.216100Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model = T5FineTuner(config)","metadata":{"id":"jiypz6jrG9bl","outputId":"d16d6835-314f-4c02-842f-0962de051474","execution":{"iopub.status.busy":"2023-08-31T17:22:36.478634Z","iopub.execute_input":"2023-08-31T17:22:36.479902Z","iopub.status.idle":"2023-08-31T17:22:50.498643Z","shell.execute_reply.started":"2023-08-31T17:22:36.479853Z","shell.execute_reply":"2023-08-31T17:22:50.497452Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# 3. Training ","metadata":{}},{"cell_type":"code","source":"trainer = pl.Trainer(**train_params,logger=wandb_logger)","metadata":{"id":"mlsnd6mu6oHn","outputId":"dfe6f12c-9dda-483b-b4e8-5f8056df362b","execution":{"iopub.status.busy":"2023-08-31T17:26:26.976316Z","iopub.execute_input":"2023-08-31T17:26:26.976740Z","iopub.status.idle":"2023-08-31T17:26:27.186634Z","shell.execute_reply.started":"2023-08-31T17:26:26.976692Z","shell.execute_reply":"2023-08-31T17:26:27.185603Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"INFO: Using 16bit Automatic Mixed Precision (AMP)\nINFO: Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.fit(model)","metadata":{"id":"iZ59dtmH6oHo","outputId":"b6530313-d689-4dc7-829e-24b7c9c333e6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Evaluation","metadata":{}},{"cell_type":"code","source":"path ='/kaggle/working/logs/epoch=5-step=2814.ckpt'","metadata":{"execution":{"iopub.status.busy":"2023-08-31T17:22:55.338806Z","iopub.execute_input":"2023-08-31T17:22:55.339892Z","iopub.status.idle":"2023-08-31T17:22:55.466694Z","shell.execute_reply.started":"2023-08-31T17:22:55.339846Z","shell.execute_reply":"2023-08-31T17:22:55.465428Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"tokenizer = T5Tokenizer.from_pretrained(config.model_name_or_path)","metadata":{"id":"QEVJPRXWgnZw","execution":{"iopub.status.busy":"2023-08-31T18:02:58.167861Z","iopub.execute_input":"2023-08-31T18:02:58.168262Z","iopub.status.idle":"2023-08-31T18:02:58.453189Z","shell.execute_reply.started":"2023-08-31T18:02:58.168233Z","shell.execute_reply":"2023-08-31T18:02:58.452082Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"data_ = HateDetect(config,tokenizer=tokenizer, part=\"test\")\nloader = DataLoader(data_, batch_size=config.eval_batch_size,drop_last=True, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2023-08-31T18:03:09.526144Z","iopub.execute_input":"2023-08-31T18:03:09.526902Z","iopub.status.idle":"2023-08-31T18:03:10.596786Z","shell.execute_reply.started":"2023-08-31T18:03:09.526859Z","shell.execute_reply":"2023-08-31T18:03:10.595475Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"outputs = []\ntargets = []\nmodel.eval()\nwith torch.no_grad():\n    for batch in tqdm(loader):\n        outs = model_test.model.generate(input_ids=batch['source_ids'].cuda(),\n                                  attention_mask=batch['source_mask'].cuda(),\n                                  max_length=2)\n\n        dec = [tokenizer.decode(ids[ids > 1 ]) for ids in outs]\n        target = [tokenizer.decode((ids[ids > 1 ])) for ids in batch[\"target_ids\"]]\n\n        outputs.extend(dec)\n        targets.extend(target)","metadata":{"id":"MEa00VKtVQq-","outputId":"1df2ef83-00d2-4fd1-ddc5-8cfe603f9a7f","execution":{"iopub.status.busy":"2023-08-31T18:03:11.821910Z","iopub.execute_input":"2023-08-31T18:03:11.822910Z","iopub.status.idle":"2023-08-31T18:03:32.418096Z","shell.execute_reply.started":"2023-08-31T18:03:11.822874Z","shell.execute_reply":"2023-08-31T18:03:32.416867Z"},"trusted":true},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/158 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b8077ee0ed74f21a47bc71bc962b333"}},"metadata":{}}]},{"cell_type":"code","source":"print(metrics.classification_report(targets, outputs))","metadata":{"id":"hCOjxCSbVSY2","outputId":"6f3cc0be-79dc-4cbb-8fba-f1bcfd7df838","execution":{"iopub.status.busy":"2023-08-31T18:03:32.569935Z","iopub.execute_input":"2023-08-31T18:03:32.570285Z","iopub.status.idle":"2023-08-31T18:03:32.771426Z","shell.execute_reply.started":"2023-08-31T18:03:32.570252Z","shell.execute_reply":"2023-08-31T18:03:32.770052Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n        hate       0.76      0.88      0.82      1056\n      normal       0.90      0.81      0.85      1472\n\n    accuracy                           0.84      2528\n   macro avg       0.83      0.84      0.83      2528\nweighted avg       0.84      0.84      0.84      2528\n\n","output_type":"stream"}]}]}