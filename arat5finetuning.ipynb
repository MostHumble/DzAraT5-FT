{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### [T5](https://github.com/google-research/text-to-text-transfer-transformer)\n- **Text-To-Text Transfer Transformer**\n- A unified framework that converts every language problem into a text-to-text format.\n- Achieves state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more.\n\n### Multi Class vs Multi Label Classification\n- **Multi Class** - There are multiple categories but each instance is assigned only one, therefore such problems are known as multi-class classification problem.","metadata":{"id":"buleZD1j2kLm"}},{"cell_type":"markdown","source":"# Imports","metadata":{"id":"vr-nvX_HT4_K"}},{"cell_type":"markdown","source":"The entire code is written using **PyTorch**.<br>\nWe'll be using the **transformers** library by [huggingface](https://github.com/huggingface/transformers) as they provide wrappers for multiple Transformer models.","metadata":{"id":"GqOpA5x73pYs"}},{"cell_type":"code","source":"%%capture\n\n!pip install transformers\n!pip install pytorch-lightning --upgrade\n!pip install sentencepiece\n!pip install datasets --upgrade\n!pip install torchmetrics\n!pip install wandb\n!pip install lightning","metadata":{"id":"9J7Ws11-9PqG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''rom google.colab import files\n\nfiles.upload()\n! mkdir ~/.kaggle\n! cp kaggle.json ~/.kaggle/\n! chmod 600 ~/.kaggle/kaggle.json\n! kaggle datasets download -d sifalklioui/hatespeechdza\n!mkdir data\n!unzip hatespeechdza.zip -d ./data'''","metadata":{"id":"61f6obyD_Kud","outputId":"63cd297c-7f38-4c70-d1fa-1b644585edc1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset as hgdataset\nfrom datasets import load_dataset\nimport numpy as np\nimport pickle\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport copy\nfrom tqdm.notebook import tqdm\nimport gc\nimport random\nimport torch\nimport wandb\nimport torchmetrics\nimport logging\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom dataclasses import dataclass\nfrom sklearn.metrics import f1_score\nimport pytorch_lightning as pl\nfrom torch.optim import AdamW\nfrom sklearn import metrics\nfrom lightning.pytorch.loggers import WandbLogger\n\nfrom transformers import (\n    T5Tokenizer,\n    T5Model,\n    T5ForConditionalGeneration,\n    get_linear_schedule_with_warmup\n)\nwandb.login(key=\"902ffdfbd80732219ee9853892860a048fa9914f\")\nwandb_logger = WandbLogger(project=\"HTarabT5\")","metadata":{"id":"07GPLpCt_AjQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass Config:\n    seed = 203\n    data_folder = \"../input/hatespeechdza\"\n    output_dir = './logs'\n    model_name_or_path = 'UBC-NLP/AraT5v2-base-1024'\n    src_max_length = 40\n    tgt_max_length = 2\n    add_special_tokens = True\n    truncation = True\n    return_tensors = 'pt'\n    padding = \"max_length\"\n    weight_decay=0.0\n    adam_epsilon=1e-8\n    warmup_steps=0\n    train_batch_size=64\n    eval_batch_size=64\n    num_train_epochs=5\n    gradient_accumulation_steps=16\n    n_gpu=1\n    fp_16= False, # if you want to enable 16-bit training then install apex and set this to true\n    max_grad_norm=1.0 # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n    learning_rate= float(3e-4)\n\nconfig = Config()","metadata":{"id":"281L6wVJ6oHj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\nset_seed(config.seed)","metadata":{"id":"8_WV69kk6oHk","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset & Dataloader","metadata":{"id":"e1AhmXnXByg5"}},{"cell_type":"markdown","source":"Now, we'll create a custom Dataset class inherited from the PyTorch Dataset class. We'll be using the **T5 tokenizer** that returns **input_ids** and **attention_mask**.<br><br>\nThe custom Dataset class will return a dict containing - <br>\n\n- src_input_ids\n- src_attention_mask\n- tgt_input_ids'\n-tgt_attention_mask","metadata":{"id":"cwrzxPV8B1Es"}},{"cell_type":"code","source":"class HateDetect():\n    def __init__(self,config,tokenizer, part):\n\n        self.config = config\n        self.part = part\n        self.tokenizer = tokenizer\n\n\n        data_paths = {\n            'train': config.data_folder + \"/dataset_prep_train.csv\",\n            'test': config.data_folder + \"/dataset_prep_test.csv\",\n            'val': config.data_folder + \"/dataset_prep_val.csv\"\n        }\n        path = data_paths.get(self.part,None)\n        if path is not None:\n            df = pd.read_csv(path)\n            df['label'].replace({0:\"normal\",1:\"hate\"}, inplace = True)\n            self.data = hgdataset.from_pandas(df ,split=self.part)\n        else:\n            raise ValueError(\"Invalid value for self.part\")\n\n\n        self.dataset_scr,self.dataset_tgt = self.tokenize()\n\n        # create funtion to tokenize data\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self,idx):\n\n        source_ids = self.dataset_scr[\"input_ids\"][idx].squeeze()\n        target_ids = self.dataset_tgt[\"input_ids\"][idx].squeeze()\n\n        src_mask    = self.dataset_scr[\"attention_mask\"][idx].squeeze()\n        target_mask = self.dataset_tgt[\"attention_mask\"][idx].squeeze()\n\n        return {\"source_ids\": source_ids,\n                \"source_mask\": src_mask,\n                \"target_ids\": target_ids,\n                \"target_mask\": target_mask}\n\n\n    def tokenize(self):\n\n        tokenizer_params = {\n            \"src\": {\n                \"max_length\": self.config.src_max_length,\n                \"add_special_tokens\": self.config.add_special_tokens,\n                \"truncation\": self.config.truncation,\n                \"return_tensors\": self.config.return_tensors,\n                \"padding\": self.config.padding\n            },\n            \"tgt\": {\n                \"max_length\": self.config.tgt_max_length,\n                \"add_special_tokens\": self.config.add_special_tokens,\n                \"truncation\": self.config.truncation,\n                \"return_tensors\": self.config.return_tensors,\n                \"padding\": self.config.padding\n            }\n        }\n        dataset_scr = self.tokenizer(self.data['text'], **tokenizer_params[\"src\"])\n        dataset_tgt = self.tokenizer(self.data['label'], **tokenizer_params[\"tgt\"])\n        return dataset_scr,dataset_tgt\n\ndef get_dataset(config,tokenizer,part):\n    return HateDetect(config,tokenizer,part)","metadata":{"id":"w0D46GkV6oHl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logger = logging.getLogger(__name__)\n\nclass DeviceCallback(pl.Callback):\n    def on_batch_start(self, trainer, pl_module):\n        assert next(pl_module.parameters()).device.type == \"cuda\"\n\nclass LoggingCallback(pl.Callback):\n    def on_validation_end(self, trainer, pl_module):\n        logger.info(\"***** Validation results *****\")\n        if pl_module.is_logger():\n            metrics = trainer.callback_metrics\n            # Log results\n            for key in sorted(metrics):\n                if key not in [\"log\", \"progress_bar\"]:\n                    logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n\n\n    ","metadata":{"id":"BGVTpK7g6oHl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    def on_test_end(self, trainer, pl_module):\n        logger.info(\"***** Validation results *****\")\n        if pl_module.is_logger():\n            metrics = trainer.callback_metrics\n            # Log results\n            for key in sorted(metrics):\n                if key not in [\"log\", \"progress_bar\"]:\n                    logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callbacks=[DeviceCallback()]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_callback = pl.callbacks.ModelCheckpoint(\n    dirpath\n    = config.output_dir, monitor=\"validation_f1_step\", mode=\"max\", save_top_k=1\n)\ntrain_params = dict(\n    devices=config.n_gpu,\n    strategy=\"auto\",\n    accelerator=\"gpu\",\n    max_epochs=config.num_train_epochs,\n    precision= \"16-mixed\",\n    gradient_clip_val=config.max_grad_norm,\n    callbacks=[DeviceCallback()]\n    \n    \n)","metadata":{"id":"U9tLgD2O6oHl","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LoggingCallback(),checkpoint_callback,","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"id":"feB5OEdeoV91"}},{"cell_type":"markdown","source":"Coming to the most interesting part - the model architecture! We'll create a class named **Model**, inherited from **torch.nn.Module**.<br><br>\n\n### Flow\n- We initialize our pretrained T5 model with a Conditional Generation Head.\n- Pass in the src & tgt, input_ids & attention_mask.\n- The model returns the decoder generated output ids (predicted labels in textual format), which we need to decode further using the tokenizer.","metadata":{"id":"HjqZhSB8C0Qk"}},{"cell_type":"code","source":"class T5FineTuner(pl.LightningModule):\n    def __init__(self, config):\n        super().__init__()\n        gc.collect()\n        torch.cuda.empty_cache() \n        self.config = config\n        self.model = T5ForConditionalGeneration.from_pretrained(config.model_name_or_path)\n        self.tokenizer = T5Tokenizer.from_pretrained(config.model_name_or_path)\n        self.training_step_outputs = []\n        self.validation_step_outputs = []\n        self.test_step_outputs = []\n        self.outputsf1 = []\n        self.targetsf1 = []\n\n    def is_logger(self):\n        return self.trainer.global_rank  <= 0\n\n    def forward(\n        self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, labels=None\n        ):\n        return self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            decoder_attention_mask=decoder_attention_mask,\n            labels=labels,\n            )\n\n    def _step(self, batch):\n        lm_labels = batch[\"target_ids\"]\n        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n        outputs = self(\n        input_ids=batch[\"source_ids\"],\n        attention_mask=batch[\"source_mask\"],\n        labels=lm_labels,\n        decoder_attention_mask=batch['target_mask']\n        )\n\n        loss = outputs[0]\n        return loss\n\n    def training_step(self, batch, batch_idx):\n        loss = self._step(batch)\n        self.log(\"train/loss\", loss)\n        self.training_step_outputs.append(loss.item())\n        return loss\n\n    def on_train_epoch_end(self):\n        \n        epoch_average = np.mean(self.training_step_outputs)\n        self.log(\"training_epoch_average\", epoch_average, sync_dist=True, prog_bar=True, logger=True, on_epoch=True)\n        self.training_step_outputs.clear()  # free memory\n\n    \n    def validation_step(self, batch, batch_idx):\n        loss = self._step(batch)\n        self.validation_step_outputs.append(loss.item())\n\n        return loss\n\n    def on_validation_epoch_end(self):\n        epoch_average = np.mean(self.validation_step_outputs)\n        \n        self.log(\"validation_epoch_average\", epoch_average, sync_dist=True, prog_bar=True, logger=True, on_epoch=True)\n        self.validation_step_outputs.clear()\n        \n\n    def configure_optimizers(self):\n        \"Prepare optimizer and schedule (linear warmup and decay)\"\n\n        model = self.model\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n\n        optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": self.config.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n        ]\n        optimizer = AdamW(optimizer_grouped_parameters, lr=self.config.learning_rate, eps=self.config.adam_epsilon)\n        scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=self.config.warmup_steps, num_training_steps=self.trainer.estimated_stepping_batches)\n        self.lr_scheduler = scheduler\n        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n        return [optimizer],[scheduler]\n\n\n\n    def train_dataloader(self):\n        train_dataset = get_dataset(config=self.config, tokenizer=self.tokenizer, part=\"train\")\n        return DataLoader(train_dataset, batch_size=self.config.train_batch_size, drop_last=True, shuffle=True,num_workers=2)\n    def val_dataloader(self):\n        val_dataset = get_dataset(config=self.config,tokenizer=self.tokenizer, part=\"val\")\n        return DataLoader(val_dataset, batch_size=self.config.eval_batch_size, drop_last=True,num_workers=2)\n","metadata":{"id":"d1xG8CCdlgge","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    def test_dataloader(self):\n        val_dataset = get_dataset(config=self.config,tokenizer=self.tokenizer, part=\"test\")\n        return DataLoader(val_dataset, batch_size=self.config.eval_batch_size,drop_last=True, num_workers=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_step(self, batch, batch_idx):\n        loss = self._step(batch)\n        self.test_step_outputs.append(loss)\n        return loss\n\ndef on_test_epoch_end(self):\n    epoch_average = torch.stack(self.test_step_outputs).mean()\n    self.log(\"test_epoch_average\", epoch_average,  sync_dist=True, prog_bar=True, logger=True, on_epoch=True)\n    self.test_step_outputs.clear()  # free memory\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"        #self.outputsf1.append(outs.detach().tolist())\n        #self.targetsf1.append(target.detach().tolist())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#target_binary = mlb.fit_transform(self.targetsf1)\n        #output_binary = mlb.transform(self.outputsf1)\n        \n        \n        #out_flat = [pair[0] for sublist in self.outputsf1 for pair in sublist]\n        #target_flat = [pair[0] for sublist in self.targetsf1 for pair in sublist]\n        #targets = [ids[0] for batch in self.targetsf1 for ids in batch]\n        #outputs = [ids[0] for batch in self.outputsf1 for ids in batch]\n        \n        #f1 = f1_score(targets,outputs,average='macro')\n        #self.log(\"validation_f1_step\",float(0.5) ,  sync_dist=True, prog_bar=True, logger=True, on_epoch=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"        outs = self.model.generate(input_ids=batch['source_ids'],\n                              attention_mask=batch['source_mask'],\n                              max_length=2)\n\n        target = batch[\"target_ids\"]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#self.targetsf1.clear()\n        #self.outputsf1.clear()\n        # free memory","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = T5FineTuner(config)","metadata":{"id":"jiypz6jrG9bl","outputId":"d16d6835-314f-4c02-842f-0962de051474","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = pl.Trainer(**train_params)","metadata":{"id":"mlsnd6mu6oHn","outputId":"dfe6f12c-9dda-483b-b4e8-5f8056df362b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.fit(model)","metadata":{"id":"iZ59dtmH6oHo","outputId":"b6530313-d689-4dc7-829e-24b7c9c333e6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = T5Tokenizer.from_pretrained(config.model_name_or_path)","metadata":{"id":"QEVJPRXWgnZw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_ = HateDetect(config,tokenizer=tokenizer, part=\"test\")\nloader = DataLoader(data_, batch_size=config.eval_batch_size,drop_last=True, num_workers=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modelb =  T5ForConditionalGeneration.from_pretrained(\"/kaggle/working/lightning_logs/version_0/checkpoints/\"+'epoch=4-step=585.ckpt'\n)\noutputs = []\ntargets = []\nfor batch in tqdm(loader):\n    outs = modelb.model.generate(input_ids=batch['source_ids'].cuda(),\n                              attention_mask=batch['source_mask'].cuda(),\n                              max_length=2)\n\n    dec = [tokenizer.decode(ids[ids > 1 ]) for ids in outs]\n    target = [tokenizer.decode((ids[ids > 1 ])) for ids in batch[\"target_ids\"]]\n\n    outputs.extend(dec)\n    targets.extend(target)","metadata":{"id":"MEa00VKtVQq-","outputId":"1df2ef83-00d2-4fd1-ddc5-8cfe603f9a7f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(metrics.classification_report(targets, outputs))","metadata":{"id":"hCOjxCSbVSY2","outputId":"6f3cc0be-79dc-4cbb-8fba-f1bcfd7df838","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import textwrap","metadata":{"id":"3gqbs7XaECCm","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(32):\n    lines = textwrap.wrap(\"Review:\\n%s\\n\" % texts[i], width=100)\n    print(\"\\n\".join(lines))\n    print(\"\\nActual sentiment: %s\" % targets[i])\n    print(\"Predicted sentiment: %s\" % dec[i])\n    print(\"=====================================================================\\n\")","metadata":{"id":"7sw2flRKD-Ee","trusted":true},"execution_count":null,"outputs":[]}]}